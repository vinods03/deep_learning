{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comprehensive Classification Problem\n",
    "\n",
    "Multi class \n",
    "Handling class imbalance\n",
    "logistic regression with hyper parameter tuning\n",
    "decision tree classifier with hyper parameter tuning\n",
    "random forest classifier with hyper parameter tuning\n",
    "SVM Classifier -> linear\n",
    "SVM Classifer -> kernel\n",
    "k neighbors classifier with hyper parameter tuning - number of neighbors\n",
    "Adaboost classifier with hyper-parameter tuning\n",
    "XGBoost Classifier with hyper-parameter tuning\n",
    "\n",
    "Note on Bias-Variance trade-off\n",
    "\n",
    "Difference w.r.t 55. yeast classification - Smote and UnderSampling is that this uses RandomOverSampling only\n",
    "We have tried RandomOverSampling without shrinkage and with shrinkage.\n",
    "Note, we have not used both one on top of another like Smote over RandomUndersampling in the other example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mcg</th>\n",
       "      <th>gvh</th>\n",
       "      <th>alm</th>\n",
       "      <th>mit</th>\n",
       "      <th>erl</th>\n",
       "      <th>pox</th>\n",
       "      <th>vac</th>\n",
       "      <th>nuc</th>\n",
       "      <th>class_protein_localization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.61</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.22</td>\n",
       "      <td>MIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.43</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.22</td>\n",
       "      <td>MIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.64</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.49</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.22</td>\n",
       "      <td>MIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.58</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.57</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.22</td>\n",
       "      <td>NUC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.42</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.54</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.22</td>\n",
       "      <td>MIT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mcg   gvh   alm   mit  erl  pox   vac   nuc class_protein_localization\n",
       "0  0.58  0.61  0.47  0.13  0.5  0.0  0.48  0.22                        MIT\n",
       "1  0.43  0.67  0.48  0.27  0.5  0.0  0.53  0.22                        MIT\n",
       "2  0.64  0.62  0.49  0.15  0.5  0.0  0.53  0.22                        MIT\n",
       "3  0.58  0.44  0.57  0.13  0.5  0.0  0.54  0.22                        NUC\n",
       "4  0.42  0.44  0.48  0.54  0.5  0.0  0.48  0.22                        MIT"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('dataset_185_yeast.csv')\n",
    "pd.set_option('display.max_columns',None)\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1484 entries, 0 to 1483\n",
      "Data columns (total 9 columns):\n",
      " #   Column                      Non-Null Count  Dtype  \n",
      "---  ------                      --------------  -----  \n",
      " 0   mcg                         1484 non-null   float64\n",
      " 1   gvh                         1484 non-null   float64\n",
      " 2   alm                         1484 non-null   float64\n",
      " 3   mit                         1484 non-null   float64\n",
      " 4   erl                         1484 non-null   float64\n",
      " 5   pox                         1484 non-null   float64\n",
      " 6   vac                         1484 non-null   float64\n",
      " 7   nuc                         1484 non-null   float64\n",
      " 8   class_protein_localization  1484 non-null   object \n",
      "dtypes: float64(8), object(1)\n",
      "memory usage: 104.5+ KB\n"
     ]
    }
   ],
   "source": [
    "# all the columns are needed\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mcg</th>\n",
       "      <th>gvh</th>\n",
       "      <th>alm</th>\n",
       "      <th>mit</th>\n",
       "      <th>erl</th>\n",
       "      <th>pox</th>\n",
       "      <th>vac</th>\n",
       "      <th>nuc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "      <td>1484.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.500121</td>\n",
       "      <td>0.499933</td>\n",
       "      <td>0.500034</td>\n",
       "      <td>0.261186</td>\n",
       "      <td>0.504717</td>\n",
       "      <td>0.007500</td>\n",
       "      <td>0.499885</td>\n",
       "      <td>0.276199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.137299</td>\n",
       "      <td>0.123924</td>\n",
       "      <td>0.086670</td>\n",
       "      <td>0.137098</td>\n",
       "      <td>0.048351</td>\n",
       "      <td>0.075683</td>\n",
       "      <td>0.057797</td>\n",
       "      <td>0.106491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.110000</td>\n",
       "      <td>0.130000</td>\n",
       "      <td>0.210000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.410000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.480000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.220000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.510000</td>\n",
       "      <td>0.220000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.570000</td>\n",
       "      <td>0.550000</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.530000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.830000</td>\n",
       "      <td>0.730000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mcg          gvh          alm          mit          erl  \\\n",
       "count  1484.000000  1484.000000  1484.000000  1484.000000  1484.000000   \n",
       "mean      0.500121     0.499933     0.500034     0.261186     0.504717   \n",
       "std       0.137299     0.123924     0.086670     0.137098     0.048351   \n",
       "min       0.110000     0.130000     0.210000     0.000000     0.500000   \n",
       "25%       0.410000     0.420000     0.460000     0.170000     0.500000   \n",
       "50%       0.490000     0.490000     0.510000     0.220000     0.500000   \n",
       "75%       0.580000     0.570000     0.550000     0.320000     0.500000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "               pox          vac          nuc  \n",
       "count  1484.000000  1484.000000  1484.000000  \n",
       "mean      0.007500     0.499885     0.276199  \n",
       "std       0.075683     0.057797     0.106491  \n",
       "min       0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.480000     0.220000  \n",
       "50%       0.000000     0.510000     0.220000  \n",
       "75%       0.000000     0.530000     0.300000  \n",
       "max       0.830000     0.730000     1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mcg                           0\n",
       "gvh                           0\n",
       "alm                           0\n",
       "mit                           0\n",
       "erl                           0\n",
       "pox                           0\n",
       "vac                           0\n",
       "nuc                           0\n",
       "class_protein_localization    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numeric & categorical columns segregation not needed as all are numeric columns\n",
    "# outlier handling not needed as all columns seem to be of the same scale\n",
    "# lets look at nulls -> no nulls as well\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CYT    463\n",
       "NUC    429\n",
       "MIT    244\n",
       "ME3    163\n",
       "ME2     51\n",
       "ME1     44\n",
       "EXC     35\n",
       "VAC     30\n",
       "POX     20\n",
       "ERL      5\n",
       "Name: class_protein_localization, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# class imbalance\n",
    "data['class_protein_localization'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features and target\n",
    "features_df = data.drop(['class_protein_localization'], axis = 1)\n",
    "features_df\n",
    "\n",
    "# target_df = pd.DataFrame(data['class_protein_localization'])\n",
    "# target_df\n",
    "\n",
    "target_df = data['class_protein_localization']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original classes CYT    463\n",
      "NUC    429\n",
      "MIT    244\n",
      "ME3    163\n",
      "ME2     51\n",
      "ME1     44\n",
      "EXC     35\n",
      "VAC     30\n",
      "POX     20\n",
      "ERL      5\n",
      "Name: class_protein_localization, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Original classes', target_df.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# without using shrinkage\n",
    "\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state = 42)\n",
    "features_df_ros, target_dt_ros = ros.fit_resample(features_df, target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After handling class imbalance:  NUC    463\n",
      "ME2    463\n",
      "MIT    463\n",
      "EXC    463\n",
      "ME3    463\n",
      "POX    463\n",
      "ERL    463\n",
      "CYT    463\n",
      "VAC    463\n",
      "ME1    463\n",
      "Name: class_protein_localization, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('After handling class imbalance: ', target_dt_ros.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1484, 8)\n",
      "(1484,)\n",
      "(4630, 8)\n",
      "(4630,)\n"
     ]
    }
   ],
   "source": [
    "print(features_df.shape)\n",
    "print(target_df.shape)\n",
    "print(features_df_ros.shape)\n",
    "print(target_dt_ros.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i looked at the 2 csv files for ERL that has only 5 values in the original dataset\n",
    "# i took one record out of these 5 i.e. mcg = 0.86 and found that in the over-sampled data, there are 109 exact records for this\n",
    "# by exact i mean all the other attributes had the same values as the original dataset\n",
    "\n",
    "original_data = features_df.join(target_df)\n",
    "original_data.to_csv('original_data.csv')\n",
    "over_sampled_data = features_df_ros.join(target_dt_ros)\n",
    "over_sampled_data.to_csv('over_sampled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using shrinkage factor\n",
    "\n",
    "ros = RandomOverSampler(random_state = 42, shrinkage = 3)\n",
    "features_df_ros, target_dt_ros = ros.fit_resample(features_df, target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After handling class imbalance with shrinkage factor :  NUC    463\n",
      "ME2    463\n",
      "MIT    463\n",
      "EXC    463\n",
      "ME3    463\n",
      "POX    463\n",
      "ERL    463\n",
      "CYT    463\n",
      "VAC    463\n",
      "ME1    463\n",
      "Name: class_protein_localization, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('After handling class imbalance with shrinkage factor : ', target_dt_ros.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now i see the newly added rows are not exact copies\n",
    "\n",
    "over_sampled_data_with_shrinkage = features_df_ros.join(target_dt_ros)\n",
    "over_sampled_data_with_shrinkage.to_csv('over_sampled_data_with_shrinkage.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Smote did not work for me\n",
    "\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# smote = SMOTE(k_neighbors=3)\n",
    "# features_df_smote, target_dt_smote = smote.fit_resample(features_df, target_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3704, 8)\n",
      "(926, 8)\n",
      "(3704,)\n",
      "(926,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(features_df_ros, target_dt_ros, train_size = 0.8, test_size = 0.2)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# naive logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(random_state = 2)\n",
    "lr.fit(X_train, y_train)\n",
    "y_train_pred = lr.predict(X_train)\n",
    "y_test_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['POX', 'MIT', 'ME2', 'ME1', 'ERL', 'ME3', 'ME1', 'ME1', 'EXC',\n",
       "       'MIT', 'EXC', 'NUC', 'ME3', 'ERL', 'ME1', 'CYT', 'ME3', 'ME3',\n",
       "       'NUC', 'EXC', 'POX', 'VAC', 'ERL', 'EXC', 'ME3', 'MIT', 'ME1',\n",
       "       'VAC', 'MIT', 'ME1', 'MIT', 'NUC', 'EXC', 'ME2', 'CYT', 'EXC',\n",
       "       'CYT', 'ERL', 'VAC', 'MIT', 'EXC', 'ME2', 'ME3', 'CYT', 'EXC',\n",
       "       'ME3', 'CYT', 'MIT', 'EXC', 'ME2', 'ERL', 'NUC', 'ME3', 'ME1',\n",
       "       'CYT', 'ERL', 'EXC', 'POX', 'NUC', 'ERL', 'ME1', 'CYT', 'ME3',\n",
       "       'EXC', 'VAC', 'MIT', 'CYT', 'POX', 'CYT', 'MIT', 'POX', 'ME2',\n",
       "       'ME3', 'ME1', 'ME1', 'ERL', 'ERL', 'ME3', 'NUC', 'VAC', 'MIT',\n",
       "       'ME2', 'POX', 'EXC', 'MIT', 'ERL', 'MIT', 'CYT', 'ME2', 'ME2',\n",
       "       'ERL', 'ERL', 'EXC', 'POX', 'ME3', 'ME3', 'ME2', 'VAC', 'POX',\n",
       "       'MIT', 'ME2', 'MIT', 'NUC', 'ME1', 'CYT', 'MIT', 'EXC', 'MIT',\n",
       "       'ERL', 'VAC', 'EXC', 'ME3', 'EXC', 'POX', 'EXC', 'EXC', 'ERL',\n",
       "       'ERL', 'ERL', 'ME3', 'ME2', 'ERL', 'NUC', 'ME1', 'CYT', 'EXC',\n",
       "       'POX', 'CYT', 'MIT', 'NUC', 'ERL', 'ME3', 'MIT', 'MIT', 'ERL',\n",
       "       'NUC', 'ME3', 'EXC', 'ME2', 'VAC', 'NUC', 'ME3', 'ME3', 'NUC',\n",
       "       'ME1', 'ERL', 'ERL', 'ERL', 'ERL', 'EXC', 'CYT', 'POX', 'VAC',\n",
       "       'ME3', 'EXC', 'NUC', 'ERL', 'NUC', 'MIT', 'CYT', 'CYT', 'POX',\n",
       "       'ME1', 'NUC', 'NUC', 'MIT', 'MIT', 'ME1', 'ERL', 'NUC', 'VAC',\n",
       "       'CYT', 'ME2', 'VAC', 'MIT', 'VAC', 'ERL', 'ME3', 'ME1', 'EXC',\n",
       "       'CYT', 'CYT', 'CYT', 'EXC', 'CYT', 'ME1', 'POX', 'CYT', 'ME3',\n",
       "       'ERL', 'MIT', 'ME1', 'CYT', 'VAC', 'ME3', 'NUC', 'VAC', 'ME3',\n",
       "       'MIT', 'POX', 'ME1', 'ME3', 'VAC', 'ME3', 'MIT', 'NUC', 'CYT',\n",
       "       'NUC', 'CYT', 'NUC', 'EXC', 'EXC', 'POX', 'EXC', 'ME2', 'EXC',\n",
       "       'MIT', 'ME3', 'ERL', 'ERL', 'EXC', 'POX', 'ME3', 'VAC', 'ME1',\n",
       "       'VAC', 'ME3', 'NUC', 'ERL', 'EXC', 'ERL', 'ME3', 'POX', 'VAC',\n",
       "       'MIT', 'EXC', 'ME3', 'EXC', 'ME1', 'MIT', 'CYT', 'EXC', 'ME1',\n",
       "       'ME2', 'NUC', 'POX', 'POX', 'VAC', 'EXC', 'POX', 'MIT', 'VAC',\n",
       "       'ME3', 'EXC', 'ME3', 'EXC', 'ME1', 'ME2', 'ME3', 'ME3', 'VAC',\n",
       "       'ERL', 'ME2', 'ME3', 'ME3', 'NUC', 'VAC', 'NUC', 'ERL', 'ME3',\n",
       "       'VAC', 'CYT', 'EXC', 'ME1', 'ME1', 'CYT', 'CYT', 'ME1', 'EXC',\n",
       "       'ERL', 'ME3', 'CYT', 'ME1', 'MIT', 'EXC', 'VAC', 'EXC', 'ME1',\n",
       "       'ERL', 'ME1', 'VAC', 'ERL', 'EXC', 'ERL', 'ME1', 'CYT', 'ME3',\n",
       "       'CYT', 'POX', 'ME3', 'MIT', 'ERL', 'ME3', 'ME3', 'ME1', 'CYT',\n",
       "       'EXC', 'NUC', 'ME1', 'ERL', 'EXC', 'MIT', 'NUC', 'NUC', 'POX',\n",
       "       'ME2', 'ME3', 'MIT', 'MIT', 'POX', 'NUC', 'CYT', 'ME1', 'ME1',\n",
       "       'ME1', 'MIT', 'ME2', 'ME3', 'EXC', 'ME1', 'POX', 'ME1', 'EXC',\n",
       "       'ME3', 'ME1', 'CYT', 'ME1', 'VAC', 'EXC', 'ME1', 'ME3', 'CYT',\n",
       "       'MIT', 'CYT', 'CYT', 'ERL', 'ERL', 'ME2', 'EXC', 'CYT', 'ME3',\n",
       "       'MIT', 'ME3', 'MIT', 'ME1', 'ERL', 'MIT', 'ME3', 'ME1', 'CYT',\n",
       "       'ME3', 'ERL', 'ME3', 'NUC', 'ERL', 'ERL', 'ME1', 'CYT', 'ME3',\n",
       "       'NUC', 'NUC', 'ERL', 'ME3', 'ERL', 'ME1', 'CYT', 'CYT', 'NUC',\n",
       "       'POX', 'ME2', 'EXC', 'CYT', 'ME2', 'MIT', 'ME3', 'ME3', 'ME1',\n",
       "       'EXC', 'CYT', 'EXC', 'ME3', 'ME2', 'NUC', 'ME3', 'MIT', 'ERL',\n",
       "       'CYT', 'ME2', 'ERL', 'CYT', 'VAC', 'CYT', 'ME3', 'EXC', 'VAC',\n",
       "       'POX', 'CYT', 'EXC', 'VAC', 'CYT', 'EXC', 'POX', 'VAC', 'CYT',\n",
       "       'ME3', 'POX', 'ME1', 'MIT', 'ME2', 'ME3', 'EXC', 'CYT', 'MIT',\n",
       "       'CYT', 'EXC', 'MIT', 'POX', 'ERL', 'ERL', 'ERL', 'VAC', 'MIT',\n",
       "       'ME1', 'EXC', 'ME1', 'ME3', 'ERL', 'ME1', 'CYT', 'POX', 'ERL',\n",
       "       'ME1', 'ERL', 'VAC', 'ME1', 'ME1', 'POX', 'NUC', 'MIT', 'ME1',\n",
       "       'CYT', 'ME3', 'ME1', 'ME2', 'NUC', 'MIT', 'NUC', 'CYT', 'ME3',\n",
       "       'ME1', 'ME3', 'ERL', 'EXC', 'NUC', 'POX', 'ERL', 'ME2', 'ERL',\n",
       "       'CYT', 'POX', 'ME1', 'ME3', 'EXC', 'ERL', 'ME2', 'MIT', 'MIT',\n",
       "       'NUC', 'EXC', 'VAC', 'CYT', 'EXC', 'ME3', 'NUC', 'ERL', 'ME3',\n",
       "       'CYT', 'MIT', 'CYT', 'ME2', 'CYT', 'CYT', 'CYT', 'MIT', 'CYT',\n",
       "       'EXC', 'ERL', 'ME3', 'EXC', 'POX', 'ME3', 'MIT', 'ERL', 'ERL',\n",
       "       'ME3', 'POX', 'NUC', 'VAC', 'ME1', 'EXC', 'MIT', 'EXC', 'CYT',\n",
       "       'CYT', 'NUC', 'NUC', 'ME3', 'EXC', 'ME1', 'ERL', 'ERL', 'EXC',\n",
       "       'ME1', 'NUC', 'EXC', 'EXC', 'ME3', 'POX', 'CYT', 'POX', 'CYT',\n",
       "       'NUC', 'POX', 'NUC', 'VAC', 'CYT', 'ERL', 'ERL', 'VAC', 'ERL',\n",
       "       'ERL', 'NUC', 'ME3', 'ERL', 'EXC', 'MIT', 'ME1', 'MIT', 'ERL',\n",
       "       'EXC', 'EXC', 'NUC', 'ERL', 'MIT', 'EXC', 'CYT', 'POX', 'ME3',\n",
       "       'ME2', 'MIT', 'ME1', 'POX', 'EXC', 'CYT', 'ERL', 'POX', 'ME1',\n",
       "       'ME1', 'ME3', 'ME1', 'ME1', 'POX', 'POX', 'ME3', 'CYT', 'ME3',\n",
       "       'CYT', 'VAC', 'ME3', 'ME1', 'CYT', 'EXC', 'NUC', 'ERL', 'ERL',\n",
       "       'ME2', 'ME1', 'ME3', 'ME2', 'ERL', 'VAC', 'ME3', 'NUC', 'ME3',\n",
       "       'CYT', 'VAC', 'NUC', 'ME3', 'POX', 'ME3', 'CYT', 'ME1', 'EXC',\n",
       "       'ME3', 'ME1', 'EXC', 'NUC', 'ME2', 'ERL', 'MIT', 'ME3', 'NUC',\n",
       "       'ERL', 'ME2', 'ME1', 'CYT', 'ERL', 'ME3', 'ME3', 'CYT', 'EXC',\n",
       "       'MIT', 'ERL', 'NUC', 'VAC', 'EXC', 'MIT', 'CYT', 'EXC', 'ERL',\n",
       "       'ME3', 'EXC', 'ME1', 'NUC', 'ME2', 'CYT', 'MIT', 'ME1', 'CYT',\n",
       "       'EXC', 'NUC', 'CYT', 'NUC', 'NUC', 'ME1', 'ERL', 'ME3', 'CYT',\n",
       "       'ERL', 'ME3', 'ME3', 'CYT', 'ME1', 'ME1', 'ME2', 'CYT', 'ME3',\n",
       "       'ME3', 'POX', 'NUC', 'ME1', 'CYT', 'MIT', 'ME1', 'CYT', 'CYT',\n",
       "       'EXC', 'MIT', 'CYT', 'ERL', 'EXC', 'ME3', 'CYT', 'ME3', 'MIT',\n",
       "       'ME3', 'MIT', 'VAC', 'ME1', 'MIT', 'ME3', 'ME1', 'ME3', 'ERL',\n",
       "       'CYT', 'CYT', 'POX', 'ERL', 'CYT', 'MIT', 'ME2', 'ME1', 'EXC',\n",
       "       'ME3', 'ERL', 'EXC', 'ME2', 'ERL', 'MIT', 'ERL', 'CYT', 'ME3',\n",
       "       'VAC', 'ME3', 'CYT', 'CYT', 'ERL', 'MIT', 'POX', 'CYT', 'MIT',\n",
       "       'NUC', 'ERL', 'ME2', 'ME2', 'ERL', 'ME1', 'NUC', 'ME1', 'ERL',\n",
       "       'CYT', 'ME3', 'POX', 'POX', 'EXC', 'ME3', 'MIT', 'ME1', 'ME2',\n",
       "       'CYT', 'ME3', 'ERL', 'NUC', 'CYT', 'NUC', 'EXC', 'NUC', 'EXC',\n",
       "       'MIT', 'ME1', 'CYT', 'ME1', 'ERL', 'POX', 'CYT', 'CYT', 'CYT',\n",
       "       'ME3', 'NUC', 'ME3', 'ME1', 'CYT', 'ERL', 'ERL', 'CYT', 'NUC',\n",
       "       'MIT', 'VAC', 'ME3', 'MIT', 'ME2', 'VAC', 'VAC', 'POX', 'ME2',\n",
       "       'ME1', 'ME3', 'NUC', 'CYT', 'ME1', 'CYT', 'EXC', 'ME3', 'VAC',\n",
       "       'ME3', 'ME1', 'VAC', 'EXC', 'POX', 'MIT', 'ERL', 'VAC', 'NUC',\n",
       "       'ME1', 'ME3', 'MIT', 'ERL', 'ME2', 'VAC', 'VAC', 'ME1', 'MIT',\n",
       "       'ME1', 'CYT', 'NUC', 'EXC', 'ME2', 'EXC', 'CYT', 'CYT', 'EXC',\n",
       "       'EXC', 'POX', 'ERL', 'EXC', 'MIT', 'CYT', 'EXC', 'ME3', 'EXC',\n",
       "       'EXC', 'ME1', 'ME2', 'ME1', 'NUC', 'ME3', 'CYT', 'ERL', 'VAC',\n",
       "       'ERL', 'EXC', 'ERL', 'CYT', 'ERL', 'ME1', 'ME3', 'CYT', 'MIT',\n",
       "       'ME2', 'ME1', 'ERL', 'ME2', 'CYT', 'ME1', 'EXC', 'ERL', 'ERL',\n",
       "       'CYT', 'VAC', 'EXC', 'ME3', 'EXC', 'VAC', 'ME1', 'CYT', 'ERL',\n",
       "       'EXC', 'NUC', 'ERL', 'ERL', 'ERL', 'ERL', 'MIT', 'ME1', 'ME3',\n",
       "       'ME3', 'VAC', 'ME1', 'NUC', 'ME2', 'EXC', 'EXC', 'ERL', 'ME1',\n",
       "       'ME3', 'ME2', 'NUC', 'CYT', 'ME3', 'ME2', 'ME1', 'NUC', 'EXC',\n",
       "       'CYT', 'POX', 'MIT', 'ERL', 'POX', 'EXC', 'MIT', 'ME3', 'MIT',\n",
       "       'ME3', 'VAC', 'NUC', 'MIT', 'ME1', 'ERL', 'MIT', 'NUC', 'MIT',\n",
       "       'POX', 'CYT', 'ME1', 'CYT', 'ME2', 'ERL', 'ME3', 'EXC', 'ME1',\n",
       "       'ERL', 'POX', 'CYT', 'CYT', 'CYT', 'ME3', 'POX', 'ME1', 'ME1',\n",
       "       'NUC', 'ERL', 'MIT', 'ME3', 'MIT', 'ME3', 'NUC', 'ME1', 'VAC',\n",
       "       'EXC', 'ME1', 'ME3', 'ME1', 'MIT', 'VAC', 'NUC', 'ERL'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred\n",
    "y_test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.5086393088552916\n",
      "error:  0.4913606911447084\n",
      "precision:  0.5016456669010374\n",
      "recall:  0.5086393088552916\n",
      "f1 score:  0.4962635852883084\n",
      "The confusion matrix is: \n",
      "[[ 44   0   2   0   0   3  12  14   0  18]\n",
      " [  0 115   0   0   0   0   0   0   0   0]\n",
      " [  8   0  38  16   8   2   6   3   0   5]\n",
      " [  0   0  25  60   9   2   1   1   0   4]\n",
      " [  2   3  19  16  18  18  10   1   0   4]\n",
      " [  1   1   0   4   1  59   5   4   0   7]\n",
      " [ 18   0   7   3   2   5  41   7   6   7]\n",
      " [ 26   0   1   0   1   4   3  38   0   3]\n",
      " [  5   0   4   5   3   6   5   0  52   3]\n",
      " [ 21   0  13   6  10  28   7  11   0   6]]\n",
      "\n",
      "\n",
      "***************** CLASSIFICATION REPORT USING LOGISTIC REGRESSION *******************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CYT       0.35      0.47      0.40        93\n",
      "         ERL       0.97      1.00      0.98       115\n",
      "         EXC       0.35      0.44      0.39        86\n",
      "         ME1       0.55      0.59      0.57       102\n",
      "         ME2       0.35      0.20      0.25        91\n",
      "         ME3       0.46      0.72      0.56        82\n",
      "         MIT       0.46      0.43      0.44        96\n",
      "         NUC       0.48      0.50      0.49        76\n",
      "         POX       0.90      0.63      0.74        83\n",
      "         VAC       0.11      0.06      0.08       102\n",
      "\n",
      "    accuracy                           0.51       926\n",
      "   macro avg       0.50      0.50      0.49       926\n",
      "weighted avg       0.50      0.51      0.50       926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "error = 1 - accuracy\n",
    "precision = metrics.precision_score(y_test, y_test_pred, average = 'weighted')\n",
    "recall = metrics.recall_score(y_test, y_test_pred, average = 'weighted')\n",
    "f1_score = metrics.f1_score(y_test, y_test_pred, average = 'weighted')\n",
    "\n",
    "# Without average = weighted, you will get below error\n",
    "# ValueError: Target is multiclass but average='binary'. \n",
    "# Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
    "\n",
    "print('accuracy: ', accuracy)\n",
    "print('error: ', error)\n",
    "print('precision: ', precision)\n",
    "print('recall: ', recall)\n",
    "print('f1 score: ', f1_score)\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "print('The confusion matrix is: ')\n",
    "print(cm)\n",
    "\n",
    "print('\\n')\n",
    "print('***************** CLASSIFICATION REPORT USING LOGISTIC REGRESSION *******************')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters using Grid Search are:  {'dual': True, 'penalty': 'l2', 'solver': 'liblinear'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "280 fits failed out of a total of 400.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1158, in fit\n",
      "    self.coef_, self.intercept_, self.n_iter_ = _fit_liblinear(\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1204, in _fit_liblinear\n",
      "    solver_type = _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py\", line 1043, in _get_liblinear_solver_type\n",
      "    raise ValueError(\n",
      "ValueError: Unsupported set of arguments: The combination of penalty='l1' and loss='logistic_regression' are not supported when dual=True, Parameters: penalty='l1', loss='logistic_regression', dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "40 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 66, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver saga supports only dual=False, got dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 66, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only dual=False, got dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 66, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only dual=False, got dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 66, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only dual=False, got dual=True\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 71, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 61, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver sag supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1091, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 78, in _check_solver\n",
      "    raise ValueError(\"penalty='none' is not supported for the liblinear solver\")\n",
      "ValueError: penalty='none' is not supported for the liblinear solver\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "10 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1101, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan 0.50918409        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.50485977        nan\n",
      " 0.50215779 0.50404968 0.50404968 0.50918409 0.50404968 0.50404968\n",
      "        nan        nan        nan        nan        nan 0.49972536\n",
      " 0.50053471        nan 0.49918482 0.49918482]\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# hyper parameter tuning for logistic regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "base_lr = LogisticRegression(random_state = 2)\n",
    "\n",
    "params = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "    'dual': [True, False],\n",
    "    'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "cv_lr = GridSearchCV(base_lr, cv = 10, param_grid = params, n_jobs = 3)\n",
    "cv_lr.fit(X_train, y_train)\n",
    "print('Best parameters using Grid Search are: ', cv_lr.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_lr.set_params(penalty = 'l2', dual = True, solver = 'liblinear')\n",
    "base_lr.fit(X_train, y_train)\n",
    "y_train_pred = lr.predict(X_train)\n",
    "y_test_pred = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.5086393088552916\n",
      "error:  0.4913606911447084\n",
      "precision:  0.5016456669010374\n",
      "recall:  0.5086393088552916\n",
      "f1 score:  0.4962635852883084\n",
      "The confusion matrix is: \n",
      "[[ 44   0   2   0   0   3  12  14   0  18]\n",
      " [  0 115   0   0   0   0   0   0   0   0]\n",
      " [  8   0  38  16   8   2   6   3   0   5]\n",
      " [  0   0  25  60   9   2   1   1   0   4]\n",
      " [  2   3  19  16  18  18  10   1   0   4]\n",
      " [  1   1   0   4   1  59   5   4   0   7]\n",
      " [ 18   0   7   3   2   5  41   7   6   7]\n",
      " [ 26   0   1   0   1   4   3  38   0   3]\n",
      " [  5   0   4   5   3   6   5   0  52   3]\n",
      " [ 21   0  13   6  10  28   7  11   0   6]]\n",
      "\n",
      "\n",
      "***************** CLASSIFICATION REPORT USING TUNED LOGISTIC REGRESSION *******************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CYT       0.35      0.47      0.40        93\n",
      "         ERL       0.97      1.00      0.98       115\n",
      "         EXC       0.35      0.44      0.39        86\n",
      "         ME1       0.55      0.59      0.57       102\n",
      "         ME2       0.35      0.20      0.25        91\n",
      "         ME3       0.46      0.72      0.56        82\n",
      "         MIT       0.46      0.43      0.44        96\n",
      "         NUC       0.48      0.50      0.49        76\n",
      "         POX       0.90      0.63      0.74        83\n",
      "         VAC       0.11      0.06      0.08       102\n",
      "\n",
      "    accuracy                           0.51       926\n",
      "   macro avg       0.50      0.50      0.49       926\n",
      "weighted avg       0.50      0.51      0.50       926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# no improvement\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "error = 1 - accuracy\n",
    "precision = metrics.precision_score(y_test, y_test_pred, average = 'weighted')\n",
    "recall = metrics.recall_score(y_test, y_test_pred, average = 'weighted')\n",
    "f1_score = metrics.f1_score(y_test, y_test_pred, average = 'weighted')\n",
    "\n",
    "# Without average = weighted, you will get below error\n",
    "# ValueError: Target is multiclass but average='binary'. \n",
    "# Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].\n",
    "\n",
    "print('accuracy: ', accuracy)\n",
    "print('error: ', error)\n",
    "print('precision: ', precision)\n",
    "print('recall: ', recall)\n",
    "print('f1 score: ', f1_score)\n",
    "\n",
    "cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "print('The confusion matrix is: ')\n",
    "print(cm)\n",
    "\n",
    "print('\\n')\n",
    "print('***************** CLASSIFICATION REPORT USING TUNED LOGISTIC REGRESSION *******************')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support vector - linear\n",
    "# no improvement over logistic regression\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model_svc = SVC(kernel = 'linear', random_state = 0)\n",
    "model_svc.fit(X_train, y_train)\n",
    "y_train_pred = model_svc.predict(X_train)\n",
    "y_test_pred = model_svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.4956803455723542\n",
      "error:  0.5043196544276458\n",
      "precision:  0.508254962246054\n",
      "recall:  0.4956803455723542\n",
      "f1 score:  0.4757581041405796\n",
      "cm: \n",
      "[[ 67   0   2   0   0   2   8   9   0   5]\n",
      " [  0 115   0   0   0   0   0   0   0   0]\n",
      " [ 11   0  38  16   6   2   6   2   0   5]\n",
      " [  1   0  26  62   6   2   1   0   0   4]\n",
      " [  5   2  15  20  16  18   6   1   0   8]\n",
      " [  5   1   0   4   2  60   3   2   0   5]\n",
      " [ 27   0   6   4   3   5  38   5   2   6]\n",
      " [ 43   1   1   0   1   1   3  24   0   2]\n",
      " [ 14   0   8   5   4   7   6   2  34   3]\n",
      " [ 29   0  13   7   9  30   4   5   0   5]]\n",
      "\n",
      "\n",
      "***************** CLASSIFICATION REPORT USING TUNED SVM LINEAR CLASSIFIER *******************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CYT       0.33      0.72      0.45        93\n",
      "         ERL       0.97      1.00      0.98       115\n",
      "         EXC       0.35      0.44      0.39        86\n",
      "         ME1       0.53      0.61      0.56       102\n",
      "         ME2       0.34      0.18      0.23        91\n",
      "         ME3       0.47      0.73      0.57        82\n",
      "         MIT       0.51      0.40      0.44        96\n",
      "         NUC       0.48      0.32      0.38        76\n",
      "         POX       0.94      0.41      0.57        83\n",
      "         VAC       0.12      0.05      0.07       102\n",
      "\n",
      "    accuracy                           0.50       926\n",
      "   macro avg       0.50      0.48      0.47       926\n",
      "weighted avg       0.51      0.50      0.48       926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "error = 1 - accuracy\n",
    "precision = metrics.precision_score(y_test, y_test_pred, average = 'weighted')\n",
    "recall = metrics.recall_score(y_test, y_test_pred, average = 'weighted')\n",
    "f1_score = metrics.f1_score(y_test, y_test_pred, average = 'weighted')\n",
    "cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print('accuracy: ', accuracy)\n",
    "print('error: ', error)\n",
    "print('precision: ', precision)\n",
    "print('recall: ', recall)\n",
    "print('f1 score: ', f1_score)\n",
    "\n",
    "print('cm: ')\n",
    "print(cm)\n",
    "\n",
    "print('\\n')\n",
    "print('***************** CLASSIFICATION REPORT USING TUNED SVM LINEAR CLASSIFIER *******************')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# support vector - rbf\n",
    "# small improvement found\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "model_svc = SVC(kernel = 'rbf', random_state = 0)\n",
    "model_svc.fit(X_train, y_train)\n",
    "y_train_pred = model_svc.predict(X_train)\n",
    "y_test_pred = model_svc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.5809935205183585\n",
      "error:  0.4190064794816415\n",
      "precision:  0.5922150003123872\n",
      "recall:  0.5809935205183585\n",
      "f1 score:  0.5629560410363366\n",
      "cm: \n",
      "[[ 70   0   1   0   0   5   8   8   0   1]\n",
      " [  0 115   0   0   0   0   0   0   0   0]\n",
      " [ 10   0  51  11   2   3   3   1   0   5]\n",
      " [  0   0  20  74   3   1   2   1   0   1]\n",
      " [  6   2  17  17  16  17   6   1   0   9]\n",
      " [  3   1   0   5   2  60   2   2   0   7]\n",
      " [ 25   0   6   7   1   7  37   4   2   7]\n",
      " [ 37   0   2   0   1   1   4  29   0   2]\n",
      " [  4   0   2   3   2   2   1   0  66   3]\n",
      " [ 20   0  12   8   4  27   4   7   0  20]]\n",
      "\n",
      "\n",
      "***************** CLASSIFICATION REPORT USING TUNED SVM rbf CLASSIFIER *******************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CYT       0.40      0.75      0.52        93\n",
      "         ERL       0.97      1.00      0.99       115\n",
      "         EXC       0.46      0.59      0.52        86\n",
      "         ME1       0.59      0.73      0.65       102\n",
      "         ME2       0.52      0.18      0.26        91\n",
      "         ME3       0.49      0.73      0.59        82\n",
      "         MIT       0.55      0.39      0.45        96\n",
      "         NUC       0.55      0.38      0.45        76\n",
      "         POX       0.97      0.80      0.87        83\n",
      "         VAC       0.36      0.20      0.25       102\n",
      "\n",
      "    accuracy                           0.58       926\n",
      "   macro avg       0.59      0.57      0.56       926\n",
      "weighted avg       0.59      0.58      0.56       926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "error = 1 - accuracy\n",
    "precision = metrics.precision_score(y_test, y_test_pred, average = 'weighted')\n",
    "recall = metrics.recall_score(y_test, y_test_pred, average = 'weighted')\n",
    "f1_score = metrics.f1_score(y_test, y_test_pred, average = 'weighted')\n",
    "cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print('accuracy: ', accuracy)\n",
    "print('error: ', error)\n",
    "print('precision: ', precision)\n",
    "print('recall: ', recall)\n",
    "print('f1 score: ', f1_score)\n",
    "\n",
    "print('cm: ')\n",
    "print(cm)\n",
    "\n",
    "print('\\n')\n",
    "print('***************** CLASSIFICATION REPORT USING TUNED SVM rbf CLASSIFIER *******************')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters using Grid search: \n",
      " {'criterion': 'entropy', 'max_features': None, 'splitter': 'best'}\n"
     ]
    }
   ],
   "source": [
    "# good improvement found -> this improvement only after handling class imbalance\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "base_dt = DecisionTreeClassifier(random_state = 10)\n",
    "\n",
    "params = {\n",
    "    'criterion': ['gini','entropy'],\n",
    "    'splitter':['random','best'],\n",
    "    'max_features':['auto','sqrt','log2',None]\n",
    "#     'max_depth':[15,30,45,None],\n",
    "#     'min_samples_split':[2,5],\n",
    "#     'min_samples_leaf':[1,3,5]\n",
    "}\n",
    "\n",
    "cv_dt = GridSearchCV(base_dt, cv = 10, param_grid = params, n_jobs = 3)\n",
    "\n",
    "cv_dt.fit(X_train, y_train)\n",
    "\n",
    "print('Best Parameters using Grid search: \\n', cv_dt.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dt.set_params(criterion = 'entropy', max_features = None, splitter = 'best', random_state = 10)\n",
    "base_dt.fit(X_train, y_train)\n",
    "y_train_pred = base_dt.predict(X_train)\n",
    "y_test_pred = base_dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.6274298056155507\n",
      "error:  0.37257019438444927\n",
      "precision:  0.6288442832861935\n",
      "recall:  0.6274298056155507\n",
      "f1_score:  0.62685349001702\n",
      "\n",
      "\n",
      "The confusion matrix is: \n",
      "[[ 38   0   1   0   1   6  16  25   1   5]\n",
      " [  0 114   0   0   0   0   0   1   0   0]\n",
      " [  6   0  41   9   4   1   5   1   2  17]\n",
      " [  1   0  13  67   3   1   2   1   0  14]\n",
      " [  3   0   4   0  61  17   0   5   0   1]\n",
      " [  2   0   1   0  19  54   0   2   0   4]\n",
      " [  9   0   2   1   0   4  52   9  15   4]\n",
      " [ 26   0   3   0   3   1   7  34   1   1]\n",
      " [  0   0   0   0   0   0   9   0  74   0]\n",
      " [  6   0  14  12   2   9   2  11   0  46]]\n",
      "\n",
      "\n",
      "***************** CLASSIFICATION REPORT USING TUNED DECISION TREE CLASSIFIER *******************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CYT       0.42      0.41      0.41        93\n",
      "         ERL       1.00      0.99      1.00       115\n",
      "         EXC       0.52      0.48      0.50        86\n",
      "         ME1       0.75      0.66      0.70       102\n",
      "         ME2       0.66      0.67      0.66        91\n",
      "         ME3       0.58      0.66      0.62        82\n",
      "         MIT       0.56      0.54      0.55        96\n",
      "         NUC       0.38      0.45      0.41        76\n",
      "         POX       0.80      0.89      0.84        83\n",
      "         VAC       0.50      0.45      0.47       102\n",
      "\n",
      "    accuracy                           0.63       926\n",
      "   macro avg       0.62      0.62      0.62       926\n",
      "weighted avg       0.63      0.63      0.63       926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "error = 1 - accuracy\n",
    "precision = metrics.precision_score(y_test, y_test_pred, average = 'weighted')\n",
    "recall = metrics.recall_score(y_test, y_test_pred, average = 'weighted')\n",
    "f1_score = metrics.f1_score(y_test, y_test_pred, average = 'weighted')\n",
    "cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print('accuracy: ', accuracy)\n",
    "print('error: ', error)\n",
    "print('precision: ', precision)\n",
    "print('recall: ', recall)\n",
    "print('f1_score: ', f1_score)\n",
    "\n",
    "print('\\n')\n",
    "print('The confusion matrix is: ')\n",
    "print(cm)\n",
    "\n",
    "print('\\n')\n",
    "print('***************** CLASSIFICATION REPORT USING TUNED DECISION TREE CLASSIFIER *******************')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Good improvement\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "base_rf = RandomForestClassifier()\n",
    "base_rf.fit(X_train, y_train)\n",
    "y_train_pred = base_rf.predict(X_train)\n",
    "y_test_pred = base_rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters using Grid Search are:  {'criterion': 'entropy', 'max_features': 'log2', 'n_estimators': 180}\n"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "             'n_estimators': [100,140,180],\n",
    "             'criterion':['gini','entropy','log_loss'],\n",
    "             'max_features':['auto','sqrt','log2',None]\n",
    "#            'max_depth':[2,3,4,5,6,7,8],\n",
    "#            'min_samples_split':[2,3,4,5,6],\n",
    "#            'min_samples_leaf':[2,3,4,5,6]\n",
    "               }\n",
    "\n",
    "base_rf = RandomForestClassifier(random_state = 10)\n",
    "cv_rf = GridSearchCV(base_rf, cv=10, param_grid=param_dist, n_jobs=3)\n",
    "cv_rf.fit(X_train, y_train)\n",
    "\n",
    "print('Best parameters using Grid Search are: ', cv_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_rf.set_params(criterion = 'entropy', max_features = 'log2', n_estimators = 180)\n",
    "base_rf.fit(X_train, y_train)\n",
    "y_train_pred = base_rf.predict(X_train)\n",
    "y_test_pred = base_rf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7181425485961123\n",
      "error:  0.28185745140388774\n",
      "precision:  0.7230298624383488\n",
      "recall:  0.7181425485961123\n",
      "f1_score:  0.7154930340087589\n",
      "\n",
      "\n",
      "The confusion matrix is: \n",
      "[[ 55   0   4   0   0   3   9  22   0   0]\n",
      " [  0 115   0   0   0   0   0   0   0   0]\n",
      " [  3   0  58  13   0   0   2   0   0  10]\n",
      " [  0   0  12  82   0   1   0   1   0   6]\n",
      " [  1   0   3   3  65  16   0   3   0   0]\n",
      " [  0   0   0   0  12  66   0   1   0   3]\n",
      " [ 15   0   2   1   1   3  50   4  17   3]\n",
      " [ 23   0   0   0   2   2   0  44   0   5]\n",
      " [  0   0   0   0   0   0   5   0  78   0]\n",
      " [  4   0  10  13   1   9   0  13   0  52]]\n",
      "\n",
      "\n",
      "***************** CLASSIFICATION REPORT USING TUNED RANDOM FOREST CLASSIFIER *******************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CYT       0.54      0.59      0.57        93\n",
      "         ERL       1.00      1.00      1.00       115\n",
      "         EXC       0.65      0.67      0.66        86\n",
      "         ME1       0.73      0.80      0.77       102\n",
      "         ME2       0.80      0.71      0.76        91\n",
      "         ME3       0.66      0.80      0.73        82\n",
      "         MIT       0.76      0.52      0.62        96\n",
      "         NUC       0.50      0.58      0.54        76\n",
      "         POX       0.82      0.94      0.88        83\n",
      "         VAC       0.66      0.51      0.57       102\n",
      "\n",
      "    accuracy                           0.72       926\n",
      "   macro avg       0.71      0.71      0.71       926\n",
      "weighted avg       0.72      0.72      0.72       926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# best so far\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "accuracy = metrics.accuracy_score(y_test, y_test_pred)\n",
    "error = 1 - accuracy\n",
    "precision = metrics.precision_score(y_test, y_test_pred, average = 'weighted')\n",
    "recall = metrics.recall_score(y_test, y_test_pred, average = 'weighted')\n",
    "f1_score = metrics.f1_score(y_test, y_test_pred, average = 'weighted')\n",
    "cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "\n",
    "print('accuracy: ', accuracy)\n",
    "print('error: ', error)\n",
    "print('precision: ', precision)\n",
    "print('recall: ', recall)\n",
    "print('f1_score: ', f1_score)\n",
    "\n",
    "print('\\n')\n",
    "print('The confusion matrix is: ')\n",
    "print(cm)\n",
    "\n",
    "print('\\n')\n",
    "print('***************** CLASSIFICATION REPORT USING TUNED RANDOM FOREST CLASSIFIER *******************')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('C:\\\\Vinod\\\\Machine_Learning_Deployment\\\\models\\\\yeast_classification_rf_model.pkl','wb') as yeast_classification_rf_model_pkl_file:\n",
    "    pickle.dump(base_rf, yeast_classification_rf_model_pkl_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.5637149028077754\n",
      "precision:  0.5746810864078601\n",
      "error:  0.43628509719222464\n",
      "recall:  0.5637149028077754\n",
      "f1 score:  0.5511498572880486\n",
      "\n",
      "\n",
      "The confusion matrix is: \n",
      "[[ 59   0   1   0   0   2   9  21   0   1]\n",
      " [  0 115   0   0   0   0   0   0   0   0]\n",
      " [ 12   0  48  10   3   2   3   4   0   4]\n",
      " [  0   0  18  68   6   3   1   1   0   5]\n",
      " [  6   1  18  14  23  15   6   3   0   5]\n",
      " [  6   1   4   4   3  53   2   2   0   7]\n",
      " [ 20   0  12   6   1   6  37   7   5   2]\n",
      " [ 34   0   1   0   1   1   3  36   0   0]\n",
      " [  4   0   2   2   1   3   1   1  69   0]\n",
      " [ 24   0  17   6   8  20   3  10   0  14]]\n",
      "\n",
      "\n",
      "***************** CLASSIFICATION REPORT USING KNeigbors CLASSIFIER *******************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CYT       0.36      0.63      0.46        93\n",
      "         ERL       0.98      1.00      0.99       115\n",
      "         EXC       0.40      0.56      0.46        86\n",
      "         ME1       0.62      0.67      0.64       102\n",
      "         ME2       0.50      0.25      0.34        91\n",
      "         ME3       0.50      0.65      0.57        82\n",
      "         MIT       0.57      0.39      0.46        96\n",
      "         NUC       0.42      0.47      0.45        76\n",
      "         POX       0.93      0.83      0.88        83\n",
      "         VAC       0.37      0.14      0.20       102\n",
      "\n",
      "    accuracy                           0.56       926\n",
      "   macro avg       0.57      0.56      0.54       926\n",
      "weighted avg       0.57      0.56      0.55       926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trying out KNN also but RF is the best\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 10) # 10 because this is the number of classes\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = knn.predict(X_train)\n",
    "y_test_pred = knn.predict(X_test)\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "accuracy = metrics.accuracy_score(y_test, y_test_pred, normalize = True)\n",
    "error = 1 - accuracy\n",
    "precision = metrics.precision_score(y_test, y_test_pred, average = 'weighted')\n",
    "recall = metrics.recall_score(y_test, y_test_pred, average = 'weighted')\n",
    "f1_score = metrics.f1_score(y_test, y_test_pred, average = 'weighted')\n",
    "\n",
    "print('accuracy: ', accuracy)\n",
    "print('precision: ', precision)\n",
    "print('error: ', error)\n",
    "print('recall: ', recall)\n",
    "print('f1 score: ', f1_score)\n",
    "\n",
    "print('\\n')\n",
    "print('The confusion matrix is: ')\n",
    "print(cm)\n",
    "\n",
    "print('\\n')\n",
    "print('***************** CLASSIFICATION REPORT USING KNeigbors CLASSIFIER *******************')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of neighbors:  1  accuracy:  0.5226781857451404\n",
      "number of neighbors:  2  accuracy:  0.5485961123110151\n",
      "number of neighbors:  3  accuracy:  0.5453563714902808\n",
      "number of neighbors:  4  accuracy:  0.5475161987041036\n",
      "number of neighbors:  5  accuracy:  0.5485961123110151\n",
      "number of neighbors:  6  accuracy:  0.5550755939524838\n",
      "number of neighbors:  7  accuracy:  0.5572354211663066\n",
      "number of neighbors:  8  accuracy:  0.5615550755939525\n",
      "number of neighbors:  9  accuracy:  0.5572354211663066\n",
      "number of neighbors:  10  accuracy:  0.5637149028077754\n",
      "number of neighbors:  11  accuracy:  0.5701943844492441\n",
      "number of neighbors:  12  accuracy:  0.5680345572354212\n",
      "number of neighbors:  13  accuracy:  0.5701943844492441\n",
      "number of neighbors:  14  accuracy:  0.5593952483801296\n",
      "number of neighbors:  15  accuracy:  0.5561555075593952\n",
      "number of neighbors:  16  accuracy:  0.562634989200864\n",
      "number of neighbors:  17  accuracy:  0.5615550755939525\n",
      "number of neighbors:  18  accuracy:  0.5561555075593952\n",
      "number of neighbors:  19  accuracy:  0.5572354211663066\n",
      "number of neighbors:  20  accuracy:  0.562634989200864\n",
      "number of neighbors:  21  accuracy:  0.5637149028077754\n",
      "number of neighbors:  22  accuracy:  0.5604751619870411\n",
      "number of neighbors:  23  accuracy:  0.5593952483801296\n",
      "number of neighbors:  24  accuracy:  0.5637149028077754\n",
      "number of neighbors:  25  accuracy:  0.5615550755939525\n",
      "number of neighbors:  26  accuracy:  0.5658747300215983\n",
      "number of neighbors:  27  accuracy:  0.5680345572354212\n",
      "number of neighbors:  28  accuracy:  0.5712742980561555\n",
      "number of neighbors:  29  accuracy:  0.5658747300215983\n",
      "number of neighbors:  30  accuracy:  0.5658747300215983\n"
     ]
    }
   ],
   "source": [
    "k_range = range(1, 31)\n",
    "accuracy_scores = []\n",
    "\n",
    "for k in k_range:\n",
    "    knn = KNeighborsClassifier(n_neighbors = k)\n",
    "    knn.fit(X_train, y_train.values.ravel())\n",
    "    y_train_pred = knn.predict(X_train)\n",
    "    y_test_pred = knn.predict(X_test)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_test_pred, normalize = True)\n",
    "    accuracy_scores.append(accuracy)\n",
    "    print('number of neighbors: ', k, ' accuracy: ', accuracy)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.5712742980561555\n",
      "precision:  0.5823414122115358\n",
      "error:  0.4287257019438445\n",
      "recall:  0.5712742980561555\n",
      "f1 score:  0.5465053360077358\n",
      "\n",
      "\n",
      "The confusion matrix is: \n",
      "[[ 70   0   2   0   0   0   6  15   0   0]\n",
      " [  0 115   0   0   0   0   0   0   0   0]\n",
      " [ 14   0  51   9   2   2   3   3   0   2]\n",
      " [  1   0  22  72   1   2   1   1   0   2]\n",
      " [ 10   2  19  16  15  17   5   1   0   6]\n",
      " [  6   1   0   3   1  61   3   4   0   3]\n",
      " [ 23   0   9   2   1   7  42   6   3   3]\n",
      " [ 40   0   0   0   1   0   3  31   0   1]\n",
      " [  5   0   4   2   1   3   1   2  65   0]\n",
      " [ 25   0  14   9   6  27   2  12   0   7]]\n",
      "\n",
      "\n",
      "***************** CLASSIFICATION REPORT USING KNeighbors CLASSIFIER *******************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CYT       0.36      0.75      0.49        93\n",
      "         ERL       0.97      1.00      0.99       115\n",
      "         EXC       0.42      0.59      0.49        86\n",
      "         ME1       0.64      0.71      0.67       102\n",
      "         ME2       0.54      0.16      0.25        91\n",
      "         ME3       0.51      0.74      0.61        82\n",
      "         MIT       0.64      0.44      0.52        96\n",
      "         NUC       0.41      0.41      0.41        76\n",
      "         POX       0.96      0.78      0.86        83\n",
      "         VAC       0.29      0.07      0.11       102\n",
      "\n",
      "    accuracy                           0.57       926\n",
      "   macro avg       0.57      0.57      0.54       926\n",
      "weighted avg       0.58      0.57      0.55       926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# n_neigbors = 1 not advisable because such a model has high variance\n",
    "# see notes below\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors = 28) \n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = knn.predict(X_train)\n",
    "y_test_pred = knn.predict(X_test)\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "accuracy = metrics.accuracy_score(y_test, y_test_pred, normalize = True)\n",
    "error = 1 - accuracy\n",
    "precision = metrics.precision_score(y_test, y_test_pred, average = 'weighted')\n",
    "recall = metrics.recall_score(y_test, y_test_pred, average = 'weighted')\n",
    "f1_score = metrics.f1_score(y_test, y_test_pred, average = 'weighted')\n",
    "\n",
    "print('accuracy: ', accuracy)\n",
    "print('precision: ', precision)\n",
    "print('error: ', error)\n",
    "print('recall: ', recall)\n",
    "print('f1 score: ', f1_score)\n",
    "\n",
    "print('\\n')\n",
    "print('The confusion matrix is: ')\n",
    "print(cm)\n",
    "\n",
    "print('\\n')\n",
    "print('***************** CLASSIFICATION REPORT USING KNeighbors CLASSIFIER *******************')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, a k-NN model fits a specific point in the data with the N nearest data points in your training set. \n",
    "For 1-NN this point depends only of 1 single other point. \n",
    "E.g. you want to split your samples into two groups (classification) - red and blue. \n",
    "If you train your model for a certain point p for which the nearest 4 neighbors would be red, blue, blue, blue (ascending by distance to p). \n",
    "Then a 4-NN would classify your point to blue (3 times blue and 1 time red), but your 1-NN model classifies it to red, because red is the nearest point. \n",
    "This means, that your model is really close to your training data and therefore the bias is low. \n",
    "If you compute the RSS between your model and your training data it is close to 0. \n",
    "In contrast to this the variance in your model is high, because your model is extremely sensitive and wiggly. \n",
    "As pointed out above, a random shuffling of your training set would be likely to change your model dramatically. \n",
    "In contrast, 10-NN would be more robust in such cases, but could be to stiff. Which k to choose depends on your data set. \n",
    "This highly depends on the Bias-Variance-Tradeoff, which exactly relates to this problem.\n",
    "\n",
    "What is bias?\n",
    "Model with high bias pays very little attention to the training data and oversimplifies the model. \n",
    "It always leads to high error on training and test data.\n",
    "\n",
    "What is variance?\n",
    "Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasnt seen before. \n",
    "As a result, such models perform very well on training data but has high error rates on test data.\n",
    "\n",
    "Low bias/ Low variance is good\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters using Grid search: \n",
      " {'algorithm': 'SAMME', 'base_estimator': RandomForestClassifier(), 'n_estimators': 70}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "param_dist = {\n",
    "             'base_estimator':[DecisionTreeClassifier(), RandomForestClassifier()],\n",
    "             'n_estimators':[70,120,180,20],\n",
    "             'algorithm':['SAMME','SAMME.R']\n",
    "             }\n",
    "\n",
    "model_adaboost = AdaBoostClassifier(random_state = 10)\n",
    "cv_model_adaboost = GridSearchCV(model_adaboost, cv = 20, param_grid = param_dist, n_jobs=3)\n",
    "cv_model_adaboost.fit(X_train,y_train)\n",
    "print('Best Parameters using Grid search: \\n', cv_model_adaboost.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.712742980561555\n",
      "precision:  0.71895042539007\n",
      "error:  0.28725701943844495\n",
      "recall:  0.712742980561555\n",
      "f1 score:  0.7096014150122245\n",
      "\n",
      "\n",
      "The confusion matrix is: \n",
      "[[ 57   0   3   0   0   1   9  21   0   2]\n",
      " [  0 115   0   0   0   0   0   0   0   0]\n",
      " [  4   0  57  11   0   1   2   2   0   9]\n",
      " [  0   0  15  80   0   2   0   1   0   4]\n",
      " [  1   0   5   2  65  16   0   1   0   1]\n",
      " [  0   1   0   0  10  69   0   1   0   1]\n",
      " [ 11   0   2   2   0   4  47   6  19   5]\n",
      " [ 26   0   1   0   2   2   1  42   0   2]\n",
      " [  0   0   0   1   0   0   5   0  77   0]\n",
      " [  8   0  11  10   1  10   1  10   0  51]]\n",
      "\n",
      "\n",
      "***************** CLASSIFICATION REPORT USING ADABOOST CLASSIFIER *******************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         CYT       0.53      0.61      0.57        93\n",
      "         ERL       0.99      1.00      1.00       115\n",
      "         EXC       0.61      0.66      0.63        86\n",
      "         ME1       0.75      0.78      0.77       102\n",
      "         ME2       0.83      0.71      0.77        91\n",
      "         ME3       0.66      0.84      0.74        82\n",
      "         MIT       0.72      0.49      0.58        96\n",
      "         NUC       0.50      0.55      0.53        76\n",
      "         POX       0.80      0.93      0.86        83\n",
      "         VAC       0.68      0.50      0.58       102\n",
      "\n",
      "    accuracy                           0.71       926\n",
      "   macro avg       0.71      0.71      0.70       926\n",
      "weighted avg       0.72      0.71      0.71       926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model_adaboost.set_params(base_estimator = RandomForestClassifier(), n_estimators = 70, algorithm = 'SAMME')\n",
    "model_adaboost.fit(X_train, y_train)\n",
    "y_train_pred = model_adaboost.predict(X_train)\n",
    "y_test_pred = model_adaboost.predict(X_test)\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "cm = metrics.confusion_matrix(y_test, y_test_pred)\n",
    "accuracy = metrics.accuracy_score(y_test, y_test_pred, normalize = True)\n",
    "error = 1 - accuracy\n",
    "precision = metrics.precision_score(y_test, y_test_pred, average = 'weighted')\n",
    "recall = metrics.recall_score(y_test, y_test_pred, average = 'weighted')\n",
    "f1_score = metrics.f1_score(y_test, y_test_pred, average = 'weighted')\n",
    "\n",
    "print('accuracy: ', accuracy)\n",
    "print('precision: ', precision)\n",
    "print('error: ', error)\n",
    "print('recall: ', recall)\n",
    "print('f1 score: ', f1_score)\n",
    "\n",
    "print('\\n')\n",
    "print('The confusion matrix is: ')\n",
    "print(cm)\n",
    "\n",
    "print('\\n')\n",
    "print('***************** CLASSIFICATION REPORT USING ADABOOST CLASSIFIER *******************')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NUC    387\n",
       "ME3    381\n",
       "POX    380\n",
       "EXC    377\n",
       "ME2    372\n",
       "CYT    370\n",
       "MIT    367\n",
       "VAC    361\n",
       "ME1    361\n",
       "ERL    348\n",
       "Name: class_protein_localization, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2748    3\n",
       "1177    0\n",
       "3278    5\n",
       "2779    3\n",
       "657     7\n",
       "Name: indexed, dtype: int32"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBoost does not seem to work on categorized target\n",
    "\n",
    "import numpy as np\n",
    "y_train_df = pd.DataFrame(y_train)\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_train_df['indexed'] = le.fit_transform(y_train_df)\n",
    "y_train_df.head(5)\n",
    "new_y_train_df = y_train_df['indexed']\n",
    "new_y_train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import xgboost as xgb\n",
    "\n",
    "# param_dist = {\n",
    "#              'booster':['gbtree', 'gblinear','dart'],\n",
    "#              'eta': [0.3,0.1],\n",
    "#              'gamma': [0,5,10],\n",
    "#              'max_depth':[6,12,18,25],\n",
    "#              'min_child_weight':[0,1,4,8],\n",
    "#              'subsample':[0,0.5,1],\n",
    "#              'tree_method':['auto', 'exact', 'approx', 'hist', 'gpu_hist']\n",
    "#              }\n",
    "\n",
    "# model_xgboost = xgb.XGBClassifier(random_state = 10)\n",
    "# cv_model_xgboost = GridSearchCV(model_xgboost, cv = 20, param_grid = param_dist, n_jobs=3)\n",
    "# cv_model_xgboost.fit(X_train,new_y_train_df)\n",
    "# print('Best Parameters using Grid search: \\n', cv_model_xgboost.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_xgboost.set_params(max_depth = 25, min_child_weight = 1, tree_method = 'approx')\n",
    "model_xgboost.fit(X_train, new_y_train_df)\n",
    "y_train_pred = model_xgboost.predict(X_train)\n",
    "y_test_pred = model_xgboost.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Vinod\\anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "y_test_df = pd.DataFrame(y_test)\n",
    "y_test_df['indexed'] = le.fit_transform(y_test_df)\n",
    "new_y_test_df = y_test_df['indexed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy:  0.7300215982721382\n",
      "precision:  0.7312437497618721\n",
      "error:  0.2699784017278618\n",
      "recall:  0.7300215982721382\n",
      "f1 score:  0.7283563304295323\n",
      "\n",
      "\n",
      "The confusion matrix is: \n",
      "[[ 52   0   0   0   0   2  13  24   0   2]\n",
      " [  0 115   0   0   0   0   0   0   0   0]\n",
      " [  3   0  60  10   0   0   2   0   0  11]\n",
      " [  0   0  13  80   0   1   0   1   0   7]\n",
      " [  0   0   1   4  70  14   0   2   0   0]\n",
      " [  1   0   0   0  13  67   0   0   0   1]\n",
      " [ 15   0   1   2   0   3  53   5  16   1]\n",
      " [ 24   0   0   0   2   2   3  42   0   3]\n",
      " [  0   0   0   0   0   0   6   0  77   0]\n",
      " [  3   0  13   9   1   8   0   8   0  60]]\n",
      "\n",
      "\n",
      "***************** CLASSIFICATION REPORT USING XGBOOST CLASSIFIER *******************\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.56      0.54        93\n",
      "           1       1.00      1.00      1.00       115\n",
      "           2       0.68      0.70      0.69        86\n",
      "           3       0.76      0.78      0.77       102\n",
      "           4       0.81      0.77      0.79        91\n",
      "           5       0.69      0.82      0.75        82\n",
      "           6       0.69      0.55      0.61        96\n",
      "           7       0.51      0.55      0.53        76\n",
      "           8       0.83      0.93      0.87        83\n",
      "           9       0.71      0.59      0.64       102\n",
      "\n",
      "    accuracy                           0.73       926\n",
      "   macro avg       0.72      0.72      0.72       926\n",
      "weighted avg       0.73      0.73      0.73       926\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# xgboost gives better results than adaboost, RF\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "cm = metrics.confusion_matrix(new_y_test_df, y_test_pred)\n",
    "accuracy = metrics.accuracy_score(new_y_test_df, y_test_pred, normalize = True)\n",
    "error = 1 - accuracy\n",
    "precision = metrics.precision_score(new_y_test_df, y_test_pred, average = 'weighted')\n",
    "recall = metrics.recall_score(new_y_test_df, y_test_pred, average = 'weighted')\n",
    "f1_score = metrics.f1_score(new_y_test_df, y_test_pred, average = 'weighted')\n",
    "\n",
    "print('accuracy: ', accuracy)\n",
    "print('precision: ', precision)\n",
    "print('error: ', error)\n",
    "print('recall: ', recall)\n",
    "print('f1 score: ', f1_score)\n",
    "\n",
    "print('\\n')\n",
    "print('The confusion matrix is: ')\n",
    "print(cm)\n",
    "\n",
    "print('\\n')\n",
    "print('***************** CLASSIFICATION REPORT USING XGBOOST CLASSIFIER *******************')\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(new_y_test_df, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
